application.title=scheduler Engine
application.description=Provides an interface to manage Jobs and Query hint configurations.
application.version=1.0.0


parameter.jobexecution.details=Job execution details containing the namespace and the job name.
jobschedulingdata.job.namespace = Namespace containing the job

job.operation.create=Create the job(s)
job.operation.create.notes=Creates the job(s) with the given name and group. This end point is invoked by SDK when a new adaptation is deployed. If this endpoint is invoked by the user without adaptation deployment, it will fail due to the missing adaptation metadata. Job will be suspended by default. It can be enabled using the cron or simple schedule(s).  
job.operation.delete=Delete the job(s)
job.operation.delete.notes=Deletes an existing job(s) identified with job name and group.This end point is also invoked by SDK during adaptation undeployment.
job.operation.cronschedule=Schedule the cron job(s)
job.operation.cronschedule.notes=Configure the job(s) identified by the job name and group, to run periodically at a given date/time specified using the cron expression in UNIX format. If the job is previously configured, same instance of the job will be updated with the specified cron expression.
job.operation.simpleschedule=Schedule the simple job(s)
job.operation.simpleschedule.notes=Configure the jobs(s) identified by the job name and group to run once at a given start date/time. If the job is previously configured, same instance of the job will be updated with the specified date/time.
job.operation.activate=Pause the job(s)
job.operation.activate.notes=Pauses jobs under the given job group. It is recommended to use the IFW trigger scripts to resume the jobs.
job.operation.execute=Executes the scheduled job. 
job.operation.abort=Abort the job(s)
job.operation.abort.notes=Abort the running job(s). This end point aborts jobs on Hive/Spark.
jobs.abort.action=Enter the list of running jobs that needs to aborted.
job.namespace = Namespace for the job
job.name=Name of the job. Format of the name is as defined by SDK during adaptation deployment.
job.group=Jobs can be grouped by giving unique group name to multiple jobs. When adaptation is deployed using SDK, all jobs for an adaptation are grouped under the same name. Format of the group name is adaptation-id_adaptation-version
job.cronexpression=Cron expression (in UNIX format) defining the interval at which the job needs to be executed periodically.
job.starttime=Date/Time (in the format yyyy.MM.dd HH:mm:ss) at which the job should be executed. If start time is empty, then job will be scheduled to execute immediately. 
jobs.action=Defines whether to pause the jobs under a given job group. Only jobs created by scheduler engine can be paused.
jobs.action.jobgroup=The job group for which the selected action is to be applied. Selected action will be applied to all the jobs in the group.
parameter.jobdescriptor = List of all the jobs identified by their name and group .
parameter.cronjobdescriptor = List of all the jobs identified by their name and group, along with the cron expression that need to be updated for the job.
parameter.simplejobdescriptor = List of all the jobs identified by their name and group, along with the start date/time for a one time execution of each job.

configmap.fetch = Fetch hint configurations
configmap.fetch.notes = Returns the complete hint configuration for the selected Query engine.
configmap.update = Update hint configurations
configmap.update.notes = Replaces the hints configured for the selected Query engine with those provided. The entire hints configuration needs to be provided in the request.
configmap.queryEngine.fetch = Query engine for which the hints are to be fetched.
configmap.queryEngine.update = Query engine for which the hints are to be updated.
configmap.querysettings = Hints needed to execute jobs on Hive or Spark 