[DirectorySection]
sparkMonitoring=/opt/nsn/ngdb/monitoring/output/sparkMonitoring/
pySparkMonitoring=/opt/nsn/ngdb/monitoring/output/sparkMonitoring/
ETLTopologyStatus=/opt/nsn/ngdb/monitoring/output/ETLTopologyStatus/
monitoringOutputDirectory=/opt/nsn/ngdb/monitoring/output/
ServiceStability=/opt/nsn/ngdb/monitoring/output/serviceStability/
DiskUsage=/opt/nsn/ngdb/monitoring/output/diskSpaceUtilization/
MemoryUsage=/opt/nsn/ngdb/monitoring/output/memoryUtilization/
CpuUsage=/opt/nsn/ngdb/monitoring/output/cpuUtilization/
JobsFailure=/opt/nsn/ngdb/monitoring/output/jobsFailure/
DataSourceReachability=/opt/nsn/ngdb/monitoring/output/dataSourceReachability/
TnpLatency=/opt/nsn/ngdb/monitoring/output/Tnp_Latency/
usageBoundary=/opt/nsn/ngdb/monitoring/output/boundaryStatus/
15minBoundary=/opt/nsn/ngdb/monitoring/output/boundaryStatus/
hourBoundary=/opt/nsn/ngdb/monitoring/output/boundaryStatus/
dayBoundary=/opt/nsn/ngdb/monitoring/output/boundaryStatus/
weekBoundary=/opt/nsn/ngdb/monitoring/output/boundaryStatus/
reaggBoundary=/opt/nsn/ngdb/monitoring/output/boundaryStatus/
postgresUsersCount=/opt/nsn/ngdb/monitoring/output/postgresUsersCount/
failedQsTable=/opt/nsn/ngdb/monitoring/output/qsCache/
expectedActualQsTable=/opt/nsn/ngdb/monitoring/output/qsCache/
tnpJobs = /opt/nsn/ngdb/monitoring/output/tnpJobStatus/
dayQS =/opt/nsn/ngdb/monitoring/output/qsJobStatus_Day/
weekQS = /opt/nsn/ngdb/monitoring/output/qsJobStatus_Week/
usageJob = /opt/nsn/ngdb/monitoring/output/Usage_Job/
usageWorkDir = /opt/nsn/ngdb/monitoring/output/work/
usageOutputDir = /opt/nsn/ngdb/monitoring/output/tableCount_Usage/
dayTableCount = /opt/nsn/ngdb/monitoring/output/tableCount_Day/
weekTableCount = /opt/nsn/ngdb/monitoring/output/tableCount_Week/
monthTableCount = /opt/nsn/ngdb/monitoring/output/tableCount_Month/
summaryReport = /opt/nsn/ngdb/monitoring/output/sendSummaryReport/
etlTopologiesLag = /opt/nsn/ngdb/monitoring/output/etlLag/
fifMinSummary = /opt/nsn/ngdb/monitoring/output/jobStatus/
hourSummary = /opt/nsn/ngdb/monitoring/output/jobStatus/
daySummary = /opt/nsn/ngdb/monitoring/output/jobStatus/
weekSummary = /opt/nsn/ngdb/monitoring/output/jobStatus/
dimensionCount = /opt/nsn/ngdb/monitoring/output/dimensionCount/
backlog = /opt/nsn/ngdb/monitoring/output/backlogHadoop/
backlogElastic = /opt/nsn/ngdb/monitoring/output/backlogHadoop/
hdfsPerformance=/opt/nsn/ngdb/monitoring/output/hdfsPerformance/
ConnectorStatus=/opt/nsn/ngdb/monitoring/output/ConnectorStatus/
sdkExportJobStats=/opt/nsn/ngdb/monitoring/output/exportJobStats/
ceiIndex=/opt/nsn/ngdb/monitoring/output/ceiIndex/
ngdbFileSystem=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
totalFileSystem=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemUsage=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemDim=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemAgg_15min=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemAgg_hour=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemAgg_day=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemAgg_week=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
fileSystemAgg_month=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
hdfsUsedPerc=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
mntStats=/opt/nsn/ngdb/monitoring/output/fileSystemUsage/
longRunningJob = /opt/nsn/ngdb/monitoring/output/jobStatus/
dataTraffic=/opt/nsn/ngdb/monitoring/output/DataTraffic/
[ColumnSection]
radioSeggTable=count(distinct imsi_id) as RADIO_SUSCRIBER
dataTraffic=count(distinct imsi_id) as TOTAL_SUSBSCRIBER; count(distinct case when NUMBER_OF_SMS > 0 then imsi_id else NULL end) as SMS_Active_Subscribers;count(distinct case when NUMBER_OF_CALLS > 0 then imsi_id else NULL end) as VOICE_Active_Subscribers;count(distinct case when nvl(VOLTE_CALL_ATTEMPTS,0) +  nvl(VOLTE_REG_ATTEMPTS,0) +  nvl(VOLTE_BEARER_ACT_EVENTS,0) + nvl(VOLTE_DEDICATED_ACT_EVENTS,0) + nvl(SRVCC_ATTEMPTS,0)+nvl(volte_re_registration_attempts,0) > 0 then imsi_id else NULL end) as VOLTE_Active_Subscribers;count(distinct case when technology = 'WiFi' then imsi_id else NULL end) as VoWIFI_Active_Subscribers;count(distinct case when DATA_CP_RECORD_COUNT > 0 then imsi_id else NULL end) as Data_CP_Active_Subscribers;count(distinct case when EVENT_COUNT > 0 then imsi_id else NULL end) as Data_UP_Active_Subscribers
[HeaderSection]
dataTraffic=Date,TOTAL_SUSBSCRIBER,SMS_Active_Subscribers,VOICE_Active_Subscribers,VOLTE_Active_Subscribers,VoWIFI_Active_Subscribers,Data_CP_Active_Subscribers,Data_UP_Active_Subscribers,RADIO_SUSCRIBER
ngdbFileSystem=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
totalFileSystem=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemDim=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemAgg_15min=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemAgg_hour=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemAgg_day=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemAgg_week=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
fileSystemAgg_month=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
hdfsUsedPerc=Time,Total Capacity(Gb),Used Capacity(Gb),Available Capacity(Gb),HDFS Used(%)
sparkMonitoring=Date,Application Id,Query,Stages,Start Time,End Time,Status,Duration
pySparkMonitoring=Date,Application Id,JobName,Start Time,End Time,Duration
ETLTopologyStatus=Date,DateInLocalTZ,Topology,Partition,InputTime,InputDelay,OutputTime,OutputDelay,UsageJob,UsageBoundary,UsageDelay
ServiceStability=Date,Service,Host,Last_Restart_Time,Uptime
DiskUsage=Time, Node, DiskName, UtilizedCapacity
MemoryUsage=Time, Node, Free Mem(Gb), BufferCached(Gb)
CpuUsage=Time, Node, User Cpu, System Cpu, Idle Cpu, Io wait
JobsFailure=Time, Job Name,Start Time, End Time, Status, Error Description
DataSourceReachability=Time,Host,Status
TnpLatency=JobName,StartTime,EndTime,TnpLatency,OverallLatency(in minutes)
dimensionCount=Date,DimensionTable,Count
15minBoundary=time_frame,jobid,maxvalue
hourBoundary=time_frame,jobid,maxvalue
dayBoundary=time_frame,jobid,maxvalue
weekBoundary=time_frame,jobid,maxvalue
reaggBoundary=time_frame,jobid,maxvalue
usageBoundary=time_frame,jobid,maxvalue,region_id
postgresUsersCount=time,username,count
failedQsTable=jobname,count
expectedActualQsTable=Date,DateInLocalTZ,TableName,ExpectedCount,ActualCount
tnpJobs = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
dayQS =Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
weekQS = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
usageJob = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
15MinJobStatus = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
hourJobStatus = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
dayJobStatus = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
weekJobStatus = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
monthJobStatus = Time,Job Name,StartTime,EndTime,ExecutionDuration,Status
etlTopologiesLag = Date,DateInLocalTZ,Connector,Lag
fifMinSummary = Hour,FminAggEndTime,TotalJobsExecuted,Zero_duration,Success,Error,Running,LastJobName
hourSummary = Hour,HourAggEndTime,TotalJobsExecuted,Zero_duration,Success,Error,Running,LastJobName
daySummary = Day,DayAggEndTime,TotalJobsExecuted,Zero_duration,Success,Error,Running,LastJobName
weekSummary = Week,WeekAggEndTime,TotalJobsExecuted,ZeroDuration,Success,Error,Running,LastJobName
backlog = Time,DataSource,datFileCountMnt,errorDatFileCountMnt,processingDatFileCountMnt,datFileCountNgdbUsage,datFileCountNgdbWork
backlogElastic = Date,DateInLocalTZ,DataSource,datFileCountMnt,errorDatFileCountMnt,processingDatFileCountMnt,datFileCountNgdbUsage,datFileCountNgdbWork
dayTableCount = Time,Date,Table,TotalCount,Column,DistinctCountOfSelectedColumn,VarFactor
weekTableCount = Time,Date,Table,TotalCount,Column,DistinctCountOfSelectedColumn,VarFactor
monthTableCount = Time,Date,Table,TotalCount,Column,DistinctCountOfSelectedColumn,VarFactor
hdfsPerformance=Date,hdfsreadtime,hdfswritetime
ConnectorStatus=Date,Name,ConnectorNode,ConnectorStatus,TaskNode,TotalTasks,RunningTasks,TaskStatus
sdkExportJobStats=Date,Export Job,No. of Files,Size
ceiIndex=Date,Presentation Name,Index Value
mntStats = Time,Total Capacity(Tb),Used Capacity(Tb),Available Capacity(Tb),MNT Used(%)
longRunningJob = Type,Job Name,Start Time,End Time,Duration
[PostgresSqlSection]
cei_index_query=select hm_json from hm_stats where hm_type='ceiIndex' and json_value(hm_json,'$.Date')='DATE' order by json_value(hm_json,'$.Presentation Name') asc
ceiTrendsResponse=select response from saiws.cache_tab where request like '%CEI Trends%' and query not like '%technology%' and table_name like '%cei2_o_index_city%day%' and source='S' and dt='Date'
failedJobsCountQuery=select count(*) as COUNT_OF_ERROR_JOBS from etl_status where status='E' and (job_name like 'AggJob' or job_name like 'ExpJob') and (start_time >= 'StartTime' or end_time > 'StartTime')
failedErrorJobs=select job_name,start_time,end_time,status,replace(error_description,',','') from etl_status where status='E' and (job_name like 'AggJob' or job_name like 'ExpJob') and (start_time >= 'StartTime' or end_time > 'StartTime')
usageBoundaryquery = select from_hr_agg_range as Time_Frame,a.jobid,maxvalue,region_id from (select boundary.jobid as jobid,maxvalue,region_id from boundary where jobid not in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='ARCHIVING_DAYS' and jp.paramvalue='0' and jd.jobid like '%Usage%')) a where a.jobid like '%Usage%' order by maxvalue desc
fifminBoundaryquery = select from_hr_agg_range ||'Hour'||'=='||to_hr_agg_range||'Hour' as Time_Frame,a.jobid,maxvalue from (select boundary.jobid as jobid,maxvalue from boundary where jobid not in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%15MIN_AggregateJob')) a where a.jobid like '%15MIN_AggregateJob' order by maxvalue desc
hourBoundaryquery = select from_hr_agg_range ||'Hour'||'=='||to_hr_agg_range||'Hour' as Time_Frame,a.jobid,maxvalue from (select boundary.jobid as jobid,maxvalue from boundary where jobid not in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%HOUR%AggregateJob%')) a where a.jobid like '%HOUR%AggregateJob%' order by maxvalue desc
dayBoundaryquery = select day_agg_date as Time_Frame,a.jobid,maxvalue from (select boundary.jobid as jobid,maxvalue from boundary where jobid not in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%DAY%AggregateJob%')) a where a.jobid like '%DAY%AggregateJob%' order by maxvalue desc
weekBoundaryquery = select 'Week('||'from_week_day'||'from_week_month'||'-'||'to_week_day'||'to_week_month'||')' as Time_Frame,a.jobid,maxvalue from (select boundary.jobid as jobid,maxvalue from boundary where jobid not in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%WEEK_AggregateJob%')) a where a.jobid like '%WEEK_AggregateJob%' order by maxvalue desc
postgresUserConnections = select date_trunc('second', now()::timestamp) as time,usename,count(*) from pg_stat_activity group by usename
15MinAggregationquery = select from_hr_agg_range||'Hour'||'=='||to_hr_agg_range||'Hour' as Time_Frame,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >'hr_agg_date_hr:00:00' and start_time <'hr_agg_date_hr:59:59' and job_name like '%15MIN_AggregateJob' order by executionduration desc
hourAggregationquery = select from_hr_agg_range||'Hour'||'=='||to_hr_agg_range||'Hour' as Time_Frame,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >'hr_agg_date:00:00' and start_time <'hr_agg_date:59:59' and job_name like '%HOUR%' order by end_time desc
dayAggregationquery = select day_agg_date as Day,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >'day_agg_min' and start_time <'day_agg_max' and (job_name like '%DAY_AggregateJob' or job_name like '%DAY_ExportJob') order by end_time desc
weekAggregationquery = select from_week_agg_range||'Week'||'=='||to_week_agg_range||'Week' as Week,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >'day_agg_date_min' and start_time <'day_agg_date_max' and job_name like '%WEEK_AggregateJob' order by end_time desc
monthAggregationquery = select job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where status='S' and start_time >'day_agg_min' and start_time <'day_agg_max' and job_name like '%MONTH_AggregateJob' order by executionduration  desc
usageJobsquery = select 'From_hour_usage_range'||'-'||'To_hour_usage_range' as Time_Frame,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status,error_description from sairepo.etl_status where start_time >'hr_from_date:00' and end_time <'hr_to_date:00' and job_name like '%Usage%' order by end_time desc
dayQSJobsquery = select day_agg_date as Day,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time )*60 + DATE_PART('seconds', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >= 'day_agg_from_date 00:00:00' and end_time < 'day_agg_to_date 23:59:00' and job_name like '%DAY_QS%' order by end_time desc
weekQSJobsquery = select From_week_agg_range||'-'||To_week_agg_range as Week,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >= 'week_agg_date 00:00:00' and job_name like '%WEEK_QS%' order by end_time desc
usageJob = select 'From_hour_usage_range'||'-'||'To_hour_usage_range' as Time_Frame,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status,error_description from sairepo.etl_status where start_time >='hr_from_date:00' and start_time < 'hr_to_date:00' and job_name like '%Usage%' order by end_time desc
fiFMinLastJobSummaryQuery = select extract(hour from start_time) as HOUR, max(end_time) as FMIN_AGG_END_TIME, count(*) as TOTAL_JOBS_EXECUTED, COUNT(case when (end_time-start_time) = '00:00:00' then job_name end) as ZERO_DURATION, COUNT(case when status = 'S' then job_name end) as SUCCESS, COUNT(case when status = 'E' then job_name end) as ERROR, COUNT(case when status = 'R' then job_name end) as RUNNING from sairepo.etl_status where job_name like '%15MIN%Aggre%' and start_time >'hr_agg_date:00:00' and end_time <'hr_agg_date:59:59' group by extract(hour from start_time) order by extract(hour from start_time)
hourLastJobSummaryQuery = select extract(hour from start_time) as HOUR, max(end_time) as HOUR_AGG_END_TIME, count(*) as TOTAL_JOBS_EXECUTED, COUNT(case when (end_time-start_time) = '00:00:00' then job_name end) as ZERO_DURATION, COUNT(case when status = 'S' then job_name end) as SUCCESS, COUNT(case when status = 'E' then job_name end) as ERROR, COUNT(case when status = 'R' then job_name end) as RUNNING from sairepo.etl_status where (job_name like '%HOUR%Aggre%' or job_name like '%HOUR_ExportJob') and start_time >'hr_agg_date:00:00' and end_time <'hr_agg_date:59:59' group by extract(hour from start_time) order by extract(hour from start_time)
dayLastJobSummaryQuery = select extract(day from start_time) as DAY, max(end_time) as DAY_AGG_END_TIME, count(*) as TOTAL_JOBS_EXECUTED, COUNT(case when (end_time-start_time) = '00:00:00' then job_name end) as ZERO_DURATION, COUNT(case when status = 'S' then job_name end) as SUCCESS, COUNT(case when status = 'E' then job_name end) as ERROR, COUNT(case when status = 'R' then job_name end) as RUNNING from sairepo.etl_status where (job_name like '%DAY%Aggre%' or job_name like '%DAY_ExportJob') and start_time >'day_agg_min' and end_time <'day_agg_max' group by extract(day from start_time) order by extract(day from start_time)
weekLastJobSummaryQuery = select extract(week from start_time) as WEEK, max(end_time) as WEEK_AGG_END_TIME, count(*) as TOTAL_JOBS_EXECUTED, COUNT(case when (end_time-start_time) = '00:00:00' then job_name end) as ZERO_DURATION, COUNT(case when status = 'S' then job_name end) as SUCCESS, COUNT(case when status = 'E' then job_name end) as ERROR, COUNT(case when status = 'R' then job_name end) as RUNNING from sairepo.etl_status where job_name like '%WEEK_AggregateJob%' and start_time >'day_agg_min' and end_time <'day_agg_max' group by extract(week from start_time) order by extract(week from start_time)
boundaryDelayedAggQuery = select maxvalue from boundary where jobid='JOB_NAME'
reaggBoundaryquery = select from_hr_agg_range||'Hour'||'=='||to_hr_agg_range||'Hour' as Time_Frame,job_name,start_time,end_time,DATE_PART('hour', end_time - start_time )* 60 + DATE_PART('minute', end_time - start_time ) ExecutionDuration,status from sairepo.etl_status where start_time >'hr_agg_date:00:00' and start_time <'hr_agg_date:59:59' and job_name like '%ReaggregateJob%' order by end_time desc
failedQsquery = select rtrim(jobname) as jobname,count(*) from qs_report_tab where qsjobstarttime > 'todaysDate 00.00.00' and status='FAILED' group by jobname
actualQsquery = select lower(table_name), count(*)  from cache_tab where source='S' and SUBSTRING_INDEX(dt,',',1) >= 'startTime' and table_name like '%input%'  group by lower(table_name),SUBSTRING_INDEX(dt,',',1) order by lower(table_name) desc
actualQsWeekquery = select lower(table_name), count(*)  from cache_tab where source='S' and (SUBSTRING_INDEX(dt,',',1) = 'startTime' or  dt=('strMonthTime,endMonthTime')) and table_name like '%input%'  group by lower(table_name) order by lower(table_name) desc
actualQsDayquery = select lower(table_name), count(*)  from cache_tab where source='S' and (SUBSTRING_INDEX(dt,',',1) = 'startTime' or  dt=('strTime,endTime') or dt=('strMonthTime,endMonthTime')) and table_name like '%input%'  group by lower(table_name) order by lower(table_name) desc
expectedQsquery = select lower(aggregation_table_name), count(*) from ws_scheduled_cache_tab where aggregation_table_name like '%input%' and is_scheduled = '1' and (ws_request not like '%<from>LAST_WEEK_DAYS_FROM</from>%' or ws_request not like '%<from>MONTH_WEEK_NO_FROM</from>%') group by lower(aggregation_table_name) order by lower(aggregation_table_name) desc
actualQsDimensionquery = select lower(SUBSTRING(table_name, 4,100)) as TableName,count(*) as ActualCount from cache_tab where source='D' and table_name like '%es_%' group by lower(SUBSTRING(table_name, 4,100)),cast(SUBSTRING_INDEX(dt,',',1) as INT) order by lower(SUBSTRING(table_name, 4,100)) desc
expectedQsDimensionquery = select TableName,count(*) as ExpectedCount from ( select distinct(trim(substring(ws_request from POSITION('</subscription>' in ws_request)))) as unique_requests , lower(SUBSTRING(SUBSTRING(job_name, 8, 45),1,length(SUBSTRING(job_name, 8, 45))-15)) as TableName from (select REPLACE(ws_request, '<groupByDateDimension>false</groupByDateDimension>', '') as ws_request ,job_name from ws_dimension_query_tab) b where job_name like '%Entity%' ) c  group by lower(TableName) order by lower(TableName) desc
exportJobLocationQuery=select a.jobid,b.paramvalue from sairepo.job_dictionary a inner join (select jobid,paramvalue from sairepo.job_prop where jobid in (select jobid from sairepo.job_prop  where jobid in (select id from sairepo.job_dictionary where jobid like '%Export%' and jobid not like '%SQM%') and paramname='HIVE_TO_HBASE_LOADER' and paramvalue='false') and paramname='EXPORT_LOCATIONS') b on a.id=b.jobid
etl_topologies=select specid, mnt_column, ngdb_column, usage_job, plevel, right2.maxvalue from (select us.specid,ac.adaptationid||'_'||ac.version||'/'||us.specid||'_'||us.version as mnt_column,ac.adaptationid||'_'||ac.version||'/'||case when aus.specid is null then us.specid else aus.specid end||'_'||case when aus.specid is null then us.version else aus.version end as ngdb_column, 'Usage_'||case when aus.specid is null then us.specid else aus.specid end||'_1_LoadJob' usage_job  from (select adaptationid as aer_adapid,usagespecid as aer_usid from adap_entity_rel where adaptationid in (select id from adapt_cata) and usagespecid is not null) join_table  join adapt_cata ac on join_table.aer_adapid=ac.id join (select * from usage_spec where (abstract is null or abstract ='TRANSIENT') and version = '1') us on join_table.aer_usid=us.id left outer join usage_spec_rel usr on usr.transientspec=us.id left outer join usage_spec aus on aus.id=usr.virtualspec and aus.version = '1' order by us.specid)left1 join (select jd.jobid job, jp.paramvalue plevel from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='PLEVEL' and jd.jobid like 'Usage%LoadJob')right1 on left1.usage_job=right1.job join (select boundary.jobid as jobid,maxvalue from boundary where jobid like '%Usage%') right2 on left1.usage_job=right2.jobid;
tableMaxvalue=select date(maxvalue) from sairepo.boundary where jobid like 'tablename';
[SummarySqlSection]
usageBoundary = select jobid as "JobName", maxvalue as "Boundary", delay as "Delay(min)", (case when maxvalue is null then 'Data Source not Configured' when (plevel like '5MIN' and delay>=35) or (plevel like '15MIN' and delay>=45) or (plevel like 'HOUR' and delay>=90) or (plevel like 'DAY' and delay>=1620) then 'Usage NOK' else 'Usage OK' end) "Status" from (select boundary.jobid, boundary.maxvalue, boundary.delay, jd.plevel from (select boundary.jobid as jobid, maxvalue, (extract('day' from (now() at time zone '{tz}'-maxvalue))*24*60 + extract('hour' from (now() at time zone '{tz}'-maxvalue))*60 + extract('minute' from (now() at time zone '{tz}'-maxvalue))) as delay from boundary where maxvalue is not null and jobid like '%Usage%LoadJob%' and jobid not like '%Usage_DQM_1_LoadJob%' ) boundary left join (select jd.jobid job, jp.paramvalue plevel from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='PLEVEL' and jd.jobid like 'Usage%LoadJob' and jd.jobid not like '%Usage_DQM_1_LoadJob%' ) jd on boundary.jobid=jd.job) as a1 order by "Status" desc;
15minBoundary = select jobid as "JobName", maxvalue as "Aggregated Till",case when now() at time zone '{tz}' < maxvalue + interval '30 minutes'  then 0 else floor(Delay) end as "Cycles Behind", (case when delay<0 then 'Disabled'  when jobid in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%15MIN_AggregateJob') then 'Disabled' else 'Enabled' end ) as "Job Availability" from (select jobid,maxvalue,(DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )*24*60 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )* 60 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes')))/15 Delay from Boundary where jobid like '%15MIN_AggregateJob%' order by maxvalue desc) as a1 order by "Cycles Behind" desc, "Job Availability" desc;
hourBoundary=select jobid as "JobName",maxvalue as "Aggregated Till", case when now() at time zone '{tz}' < maxvalue + interval '2 hours'  then 0 else floor(Delay) end as "Cycles Behind", (case when delay<0 then 'Disabled'  when jobid in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%HOUR%AggregateJob') then 'Disabled' else 'Enabled' end ) as "Job Availability" from (select jobid,maxvalue,(DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 hour') )*24 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 hour') ) + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 hour'))/60) Delay from Boundary where jobid like '%HOUR%AggregateJob%' order by maxvalue desc) as a1 order by "Cycles Behind" desc, "Job Availability" desc;
dayBoundary=select jobid as "JobName", maxvalue as "Aggregated Till", case when now() at time zone '{tz}' < maxvalue + interval '2 days'  then 0 else floor(Delay) end as "Cycles Behind", (case when delay<0 then 'Disabled'  when jobid in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%DAY%AggregateJob') then 'Disabled' else 'Enabled' end ) as "Job Availability" from (select jobid,maxvalue,(DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 days') ) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 days') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 days'))/(24*60)) Delay from Boundary where jobid like '%DAY%AggregateJob%' order by maxvalue desc) as a1 order by "Cycles Behind" desc, "Job Availability" desc;
weekBoundary=select jobid as "JobName", maxvalue as "Aggregated Till", case when now() at time zone '{tz}' < maxvalue + interval '2 weeks'  then 0 else floor(Delay) end as "Cycles Behind", (case when delay<0 then 'Disabled'  when jobid in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%WEEK%AggregateJob') then 'Disabled' else 'Enabled' end ) as "Job Availability" from (select jobid,maxvalue,(DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '7 days') ) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '7 days') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '7 days'))/(24*60))/7 Delay from Boundary where jobid like '%WEEK%AggregateJob%' order by maxvalue desc) as a1 order by "Cycles Behind" desc, "Job Availability" desc;
monthBoundary=select jobid as "JobName", maxvalue as "Aggregated Till", case when now() at time zone '{tz}' < maxvalue + interval '2 months'  then 0 else floor(Delay) end as "Cycles Behind",(case when delay<0 then 'Disabled'  when jobid in (select jd.jobid from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jp.paramvalue='NO' and jd.jobid like '%MONTH%AggregateJob') then 'Disbaled' else 'Enabled' end ) as "Job Availability"  from (select jobid,maxvalue,(DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 month')) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 month') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 month'))/(24*60))/30 Delay from Boundary where jobid like '%MONTH_AggregateJob%' order by maxvalue desc) as a1 order by "Cycles Behind" desc, "Job Availability" desc;
etlprocessing = select topology as "Topology",input_time as "Last arrival time from Mediation",input_delay as "Delay@ETL Input(min)",output_time as "Last processed time@ETL",output_delay as "Delay@ETL Output(min)",partitionlevel as "Partition Level",(case when (partitionlevel like '5MIN' and input_delay>=1440) or (partitionlevel like '15MIN' and input_delay>=1440) or (partitionlevel like 'HOUR' and input_delay>=1440) or (partitionlevel like 'DAY' and input_delay>=4320) then 'Datasource not configured' when (partitionlevel like '5MIN' and input_delay>=60) or (partitionlevel like '15MIN' and input_delay>=90) or (partitionlevel like 'HOUR' and input_delay>=180) or (partitionlevel like 'DAY' and input_delay>=2880) then 'Topology NOK'  when (partitionlevel like '5MIN' and output_delay>=20) or (partitionlevel like '15MIN' and output_delay>=30) or (partitionlevel like 'HOUR' and output_delay>=75) or (partitionlevel like 'DAY' and output_delay>=1620) then 'Topology NOK' else 'Topology OK' end) as "Remarks" from (select json_value(hm_json,'$.Topology') topology ,json_value(hm_json,'$.Partition') partitionlevel, json_value(hm_json,'$.InputTime') input_time,cast(json_value(hm_json,'$.InputDelay') as double) input_delay , json_value(hm_json,'$.OutputTime') output_time,cast(json_value(hm_json,'$.OutputDelay') as double) output_delay from hm_stats where hm_type='ETLTopologyStatus' and hm_date >'{last15_date}') as a1 order by Remarks asc;
hdfsFileSystem=select json_value(hm_json,'$.Directory') as "Directory",cast(json_value(hm_json,'$.Usage_with_replication(GB)') as double) "Usage_with_replication(Gb)" ,cast(json_value(hm_json,'$.Usage_without_replication(GB)') as double) "Usage_without_replication(Gb)" from hm_stats where hm_type='totalFileSystem' and hm_date > '{todays_date}';
connectorStatus=select json_value(hm_json,'$.Date') "Date",json_value(hm_json,'$.Name') "Connector" ,case when (cast(json_value(hm_json,'$.ConnectorStatus') as integer)) = 0 then "OK" else "NOK" end as "Connector Status", case when (cast(json_value(hm_json,'$.TaskStatus') as integer)) = 0 then "OK"  when (cast(json_value(hm_json,'$.TaskStatus') as integer)) = 1 then "Partially OK" else "NOK" end as "Tasks Status" , cast(json_value(hm_json,'$.TotalTasks') as integer) "Total Tasks", cast(json_value(hm_json,'$.RunningTasks') as integer) "Active Tasks" from hm_stats where hm_type='ConnectorStatus' and hm_date > '{last15_date}' order by (cast(json_value(hm_json,'$.ConnectorStatus') as integer))desc,(cast(json_value(hm_json,'$.TaskStatus') as integer))desc;
backlog=SELECT json_value(hm_json,'$.DateInLocalTZ') "Date", json_value(hm_json,'$.DataSource') "DataSource", cast(json_value(hm_json,'$.datFileCountMnt') AS UNSIGNED INTEGER) "ETL Backlog",cast(json_value(hm_json,'$.datFileCountNgdbUsage') AS INTEGER) "Usage Backlog" from hm_stats where hm_type='backlog' and json_value(hm_json,'$.DateInLocalTZ') > '{last15_date}' order by "ETL Backlog" desc,"Usage Backlog" desc;
etllag=SELECT json_value(hm_json,'$.DateInLocalTZ') "Date", json_value(hm_json,'$.Connector') "Topology", cast(json_value(hm_json,'$.Lag') AS double) "Lag" FROM hm_stats where hm_type='lagcount' and hm_date > '{last15_date}' order by Lag desc;
failedJobs=select (case when job_name like '%Entity%' then 'DIMENSION' when job_name like '%Usage%' then 'USAGE' when job_name like '%15MIN%' then '15MIN' when job_name like '%HOUR%' then 'HOUR' when job_name like '%DAY%' then 'DAY' when job_name like '%WEEK%' then 'WEEK' when job_name like '%MONTH%' then 'MONTH' when job_name like '%Export%' then 'Export'  when job_name like '%QSJob%' then 'QS' end)  as "Type", job_name as "Job Name", start_time as "Start Time", end_time as "End Time" from sairepo.etl_status where proc_id in (select max(proc_id) proc_id from sairepo.etl_status where (job_name like '%AggregateJob%' or job_name like '%Entity%Correlation%' or job_name like '%Usage%' or job_name like '%Export%' or job_name like '%QSJob%' and status!='W') and start_time >= '{todays_date}' group by job_name) and status = 'E';
tnpJobs=select jobid as "JobName", CONCAT(cast (maxvalue as TEXT),' TO ',cast (maxvalue + interval '15 minutes' as TEXT)) as "Aggregated Till",case when now() at time zone '{tz}' <= maxvalue + interval '45 minutes'  then 0 else round(Delay-1) end as "Cycles Behind" from (select jobid,maxvalue,(DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )*24*60 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )* 60 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes')))/15 Delay from Boundary where jobid in (select jobid from boundary where jobid like '%TNP%') order by maxvalue desc) as a1 where a1.Delay>=0 order by "Cycles Behind" desc;
15minExport=select jobid as "Job Name" , interval as "Interval", maxvalue as "Exported Till", (case when delay<0 then 0 when interval = '15MIN' and now() at time zone '{tz}' < maxvalue + interval '30 minutes'  then 0 when interval = 'HOUR' and  now() at time zone '{tz}' < maxvalue + interval '2 hours'  then 0 when interval = 'DAY' and now() at time zone '{tz}' < maxvalue + interval '2 days'  then 0 when interval = 'WEEK' and now() at time zone '{tz}' < maxvalue + interval '2 weeks' then 0 when interval = 'MONTH' and now() at time zone '{tz}' < maxvalue + interval '2 months'  then 0 else floor(delay) end) "Cycles Behind", (case when delay<0 then 'Disabled' when paramvalue = 'YES' then 'Enabled' else 'Disabled' end) "Availibility" from (select jobid, maxvalue, (case when paramvalue is null or paramvalue='' then 'YES' else paramvalue end) as paramvalue, interval, (case when interval = '15MIN' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )*24*60 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )* 60 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes')))/15 when interval = 'HOUR' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 hour') )*24 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 hour') ) + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 hour'))/60) when interval = 'DAY' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 days') ) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 days') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 days'))/(24*60)) when interval = 'WEEK' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '7 days') ) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '7 days') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '7 days'))/(24*60))/7 when interval = 'MONTH' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 month')) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 month') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 month'))/(24*60))/30 else -1 end) delay  from (select jobid as jobid, maxvalue from boundary where jobid like '%15MIN_ExportJob') boundary left join (select jd.jobid job, jp.paramvalue as paramvalue from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and jd.jobid like '%15MIN_ExportJob') jd on boundary.jobid=jd.job left join (select jd.jobid job, jp.paramvalue as interval from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='INTERVAL' and jd.jobid like '%15MIN_ExportJob') jd2 on boundary.jobid=jd2.job) as a1 order by "Cycles Behind" desc;
otherExport=select (case when jobid like '%HOUR%' then 'Hour' when jobid like '%DAY%' then 'DAY' when jobid like '%WEEK%' then 'WEEK' when jobid like '%MONTH%' then 'Month' end) "Type", jobid as "Job Name", interval as "Interval", maxvalue as "Exported Till", (case when delay<0 then 0 when interval = '15MIN' and now() at time zone '{tz}' < maxvalue + interval '30 minutes'  then 0 when interval = 'HOUR' and  now() at time zone '{tz}' < maxvalue + interval '2 hours'  then 0 when interval = 'DAY' and now() at time zone '{tz}' < maxvalue + interval '2 days'  then 0 when interval = 'WEEK' and now() at time zone '{tz}' < maxvalue + interval '2 weeks' then 0 when interval = 'MONTH' and now() at time zone '{tz}' < maxvalue + interval '2 months'  then 0 else floor(delay) end) "Cycles Behind", (case when delay<0 then 'Disabled' when paramvalue = 'YES' then 'Enabled' else 'Disabled' end) "Availibility" from (select jobid, maxvalue, (case when paramvalue is null or paramvalue='' then 'YES' else paramvalue end) as paramvalue, interval, (case when interval = '15MIN' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )*24*60 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes') )* 60 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '15 minutes')))/15 when interval = 'HOUR' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 hour') )*24 + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 hour') ) + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 hour'))/60) when interval = 'DAY' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 days') ) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 days') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 days'))/(24*60)) when interval = 'WEEK' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '7 days') ) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '7 days') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '7 days'))/(24*60))/7 when interval = 'MONTH' then (DATE_PART('day', (now() at time zone '{tz}') - (maxvalue + interval '1 month')) + DATE_PART('hour', (now() at time zone '{tz}') - (maxvalue + interval '1 month') )/24 + DATE_PART('minute', (now() at time zone '{tz}') - (maxvalue + interval '1 month'))/(24*60))/30 else -1 end) delay  from (select jobid as jobid, maxvalue from boundary where (jobid like '%HOUR_ExportJob' or jobid like '%DAY_ExportJob' or jobid like '%WEEK_ExportJob' or jobid like '%MONTH_ExportJob')) boundary left join (select jd.jobid job, jp.paramvalue as paramvalue from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='JOB_ENABLED' and (jd.jobid like '%HOUR_ExportJob' or jd.jobid like '%DAY_ExportJob' or jd.jobid like '%WEEK_ExportJob' or jd.jobid like '%MONTH_ExportJob')) jd on boundary.jobid=jd.job left join (select jd.jobid job, jp.paramvalue as interval from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='INTERVAL' and (jd.jobid like '%HOUR_ExportJob' or jd.jobid like '%DAY_ExportJob' or jd.jobid like '%WEEK_ExportJob' or jd.jobid like '%MONTH_ExportJob')) jd2 on boundary.jobid=jd2.job) as a1 order by "Cycles Behind" desc;
[hiveSqlSection]
usageTablecountquery = select hour(from_utc_timestamp(from_unixtime(floor(CAST(dt AS BIGINT)/1000), 'yyyy-MM-dd HH:mm:ss.SSS'),"TZ_VAR")) as created_timestamp,count(*)  from usageTable where dt>='minDt' and dt<='maxDt' group by hour(from_utc_timestamp(from_unixtime(floor(CAST(dt AS BIGINT)/1000), 'yyyy-MM-dd HH:mm:ss.SSS'),"TZ_VAR"))  order by created_timestamp
showtablesquery = show tables
columnquery = show columns from tablename
totalCountQuery = select count(*) from tablename where dt='startpartition'
columnCountQuery = select count(*), count(distinct column) from tablename where dt='startpartition'
dimensionCountQuery = select count(*) from tablename
hiveHints = SET mapreduce.job.queuename=root.nokia.ca4ci.monitoring,set mapred.reduce.tasks=2
showtablesdimension = show tables like '*es*'
[mariadbSqlSection]
queueInfo=select json_value(hm_json,'$.Vcore') from hm_stats where hm_type='queueUsage' and (hm_date>='minDate' and hm_date<='maxDate') and json_value(hm_json,'$.queuename')='queueName'
tableExistsQuery= SELECT * FROM information_schema.tables WHERE table_schema = 'monitoring' AND table_name = 'hm_stats';
tableCreationQuery= CREATE TABLE hm_stats (HM_DATE TIMESTAMP, HM_TYPE VARCHAR(50), HM_JSON JSON);
indexCreationQuery= CREATE INDEX hm_st_idx ON hm_stats (hm_date, hm_type);
eventSchedulerStartQuery=SET GLOBAL event_scheduler = ON;
eventExistsQuery= SELECT * FROM information_schema.EVENTS where event_name='EVENTNAME';
etlBacklogEventQuery= CREATE EVENT backlog ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),2) AND hm_type='backlog';
etlLagEventQuery = CREATE EVENT lagcount ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),2) AND hm_type='lagcount';
dimensionCountEventQuery = CREATE EVENT dimensionCount ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='dimensionCount';
usageCountEventQuery = CREATE EVENT usageCount ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='usageTableCount';
dayCountEventQuery = CREATE EVENT dayCount ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='dayTableCount';
weekCountEventQuery = CREATE EVENT weekCount ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='weekTableCount';
qsCountDayEventQuery = CREATE EVENT qsCountDay ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),7) AND hm_type='qscountDAY';
qsCountWeekEventQuery = CREATE EVENT qsCountWeek ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='qscountWEEK';
qsCountDimensionEventQuery = CREATE EVENT qsCountDimension ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),7) AND hm_type='qscountDIMENSION';
couchbaseBucketStatsEventQuery = CREATE EVENT bucketStats ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='bucketStats';
ceiIndexEventQuery = CREATE EVENT ceiIndex ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='ceiIndex';
connectorStatusEventQuery = CREATE EVENT connectorStatus ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='ConnectorStatus';
queueUsageEventQuery = CREATE EVENT queueUsage ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='queueUsage';
bucketStatsEventQuery = CREATE EVENT bucketStats ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='bucketStats';
sdkExportJobsEventQuery = CREATE EVENT sdkExportJobStats ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='sdkExportJobStats';
monthTableCountEventQuery = CREATE EVENT monthTableCount ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),60) AND hm_type='monthTableCount';
couchBaseNodeEventQuery = CREATE EVENT nodeStats ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='nodeStats';
couchbaseBucketRamEventQuery = CREATE EVENT bucketRamStats ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='bucketRamStats';
usageTableCountGraphEventQuery = CREATE EVENT usageTableCountGraph ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='usageTableCountGraph';
pySparkMonitoringEventQuery = CREATE EVENT pySparkMonitoring ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='pySparkMonitoring';
sparkMonitoringEventQuery = CREATE EVENT sparkMonitoring ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='sparkMonitoring';
HdfsUsedPercEventQuery = CREATE EVENT HdfsUsedPerc ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='HdfsUsedPerc';
ngdbFileSystemEventQuery = CREATE EVENT ngdbFileSystem ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='ngdbFileSystem';
totalFileSystemEventQuery = CREATE EVENT totalFileSystem ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='totalFileSystem';
fileSystemUsageEventQuery = CREATE EVENT fileSystemUsage ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='fileSystemUsage';
fileSystemDimEventQuery = CREATE EVENT fileSystemDim ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='fileSystemDim';
fileSystemAgg_15minEventQuery = CREATE EVENT fileSystemAgg_15min ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='fileSystemAgg_15min';
fileSystemAgg_hourEventQuery = CREATE EVENT fileSystemAgg_hour ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='fileSystemAgg_hour';
fileSystemAgg_dayEventQuery = CREATE EVENT fileSystemAgg_day ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),15) AND hm_type='fileSystemAgg_day';
fileSystemAgg_weekEventQuery = CREATE EVENT fileSystemAgg_week ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),30) AND hm_type='fileSystemAgg_week';
fileSystemAgg_monthEventQuery = CREATE EVENT fileSystemAgg_month ON SCHEDULE EVERY 1 DAY  ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<subdate(date_format(NOW(),'%Y-%m-%d 00:00:00'),45) AND hm_type='fileSystemAgg_month';
mntStatsEventQuery = CREATE EVENT mntStats ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),2) AND hm_type='mntStats';
adaptationInfoEventQuery = CREATE EVENT adaptationsInfo ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),2) AND hm_type='adaptationsInfo';
udfsInfoEventQuery = CREATE EVENT udfsInfo ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),2) AND hm_type='udfsInfo';
platformAppsInfoEventQuery = CREATE EVENT platformAppsInfo ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),2) AND hm_type='platformAppsInfo';
versionInfoEventQuery = CREATE EVENT versionInfo ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),2) AND hm_type='versionInfo';
bapdInfoEventQuery = CREATE EVENT bapdInfo ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),15) AND hm_type='Bapd';
dataTraffic = CREATE EVENT dataTraffic ON SCHEDULE EVERY 1 DAY ON COMPLETION PRESERVE DO DELETE FROM hm_stats WHERE hm_date<Subdate(date_format(Now(),'%Y-%m-%d 00:00:00'),7) AND hm_type='dataTraffic';