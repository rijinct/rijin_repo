[DirectorySection]
HdfsUsage = /var/local/monitoring/output/HdfsUsage/
TotalFileSystemUsage=/var/local/monitoring/output/HdfsUsage/
NgdbFileSystem=/var/local/monitoring/output/HdfsUsage/
ServiceStability=/var/local/monitoring/output/serviceStability/
PortalStability=/var/local/monitoring/output/serviceStability/
NameNodeStats=/var/local/monitoring/output/serviceStability/
AppNodesStats=/var/local/monitoring/output/serviceStability/
DataNodeStats=/var/local/monitoring/output/serviceStability/
ServiceStats=/var/local/monitoring/output/serviceStability/
PortalStats=/var/local/monitoring/output/serviceStability/
DataTraffic=/var/local/monitoring/output/DataTraffic/
TopologyStats=/var/local/monitoring/output/TopologyStats/
SourceSinkTasksStatus=/var/local/monitoring/output/SourceSinkTasksStatus/
Nodes_Status=/var/local/monitoring/output/Nodes_Status/
CheckTimeSpansInFile=/var/local/monitoring/output/CheckTimeSpansInFile/
etlAdaptationProp=/var/local/monitoring/output/etlAdaptationProp/
TopologiesStatusOnNodes=/var/local/monitoring/output/TopologiesStatusOnNodes/
NgdbFileSystemUsage=/var/local/monitoring/output/NgdbFileSystemUsage/
NgdbEsFileSystemUsage=/var/local/monitoring/output/NgdbEsFileSystemUsage/
NgdbPsFileSystemUsage_15MIN=/var/local/monitoring/output/NgdbPsFileSystemUsage/
NgdbPsFileSystemUsage_HOUR=/var/local/monitoring/output/NgdbPsFileSystemUsage/
NgdbPsFileSystemUsage_DAY=/var/local/monitoring/output/NgdbPsFileSystemUsage/
NgdbPsFileSystemUsage_WEEK=/var/local/monitoring/output/NgdbPsFileSystemUsage/
NgdbPsFileSystemUsage_MONTH=/var/local/monitoring/output/NgdbPsFileSystemUsage/
HbaseFileSystemUsage=/var/local/monitoring/output/HbaseFileSystemUsage/
HbaseWALsFileSystemUsage=/var/local/monitoring/output/HbaseFileSystemUsage/
Hbasehbase-stagingFileSystemUsage=/var/local/monitoring/output/HbaseFileSystemUsage/
DiskUsage=/var/local/monitoring/output/diskSpaceUtilization/
MemoryUsage=/var/local/monitoring/output/memoryUtilization/
AllNodesFileSystemUsage=/var/local/monitoring/output/AllNodesFileSystemUsage/
ApplicationNodesFileSysUse=/var/local/monitoring/output/ApplicationNodesFileSysUse/
CpuUsage=/var/local/monitoring/output/cpuUtilization/
JobsFailure=/var/local/monitoring/output/jobsFailure/
DataSourceReachability=/var/local/monitoring/output/dataSourceReachability/
TnpLatency=/var/local/monitoring/output/Tnp_Latency/
DimensionCount=/var/local/monitoring/output/Dimension_Count/
boundaryStatus=/var/local/monitoring/output/boundaryStatus/
boundaryStatusUsage=/var/local/monitoring/output/boundaryStatus/
boundaryStatusReagg=/var/local/monitoring/output/boundaryStatus/
postgresUsersCount=/var/local/monitoring/output/postgresUsersCount/
failedQsTable=/var/local/monitoring/output/qsCache/
expectedActualQsTable=/var/local/monitoring/output/qsCache/
openFiles=/var/local/monitoring/output/openFiles/
openFilesDebug=/var/local/monitoring/output/openFiles/
openFilesDetails=/var/local/monitoring/output/openFiles/
etlTopologiesLag=/var/local/monitoring/output/topology_status/
hdfsPerformance=/var/local/monitoring/output/hdfsPerformance/
backlog=/var/local/monitoring/output/backlogHadoop/
dayTableCount=/var/local/monitoring/output/tableCount_Day/
monthTableCount=/var/local/monitoring/output/tableCount_Month/
weekTableCount=/var/local/monitoring/output/tableCount_Week/
usageTableCount=/var/local/monitoring/output/tableCount_Usage/
couchBaseStats=/var/local/monitoring/output/couchBaseStats/
couchBaseRamSummary=/var/local/monitoring/output/couchBaseStats/
couchBaseCpuSummary=/var/local/monitoring/output/couchBaseStats/
webserviceMemory=/var/local/monitoring/output/webserviceMemory/
queriesPerClient=/var/local/monitoring/output/queriesPerClient/
portalConnections=/var/local/monitoring/output/portalConnections/
portalTomcat=/var/local/monitoring/output/tomcatMemory/
webserviceTomcat=/var/local/monitoring/output/tomcatMemory/
portalMemory=/var/local/monitoring/output/portalMemory/
portalQueries=/var/local/monitoring/output/portalQueries/
portalNoDataErrors=/var/local/monitoring/output/portalNoDataErrors/
portalAuditLogErrors=/var/local/monitoring/output/portalAuditLogErrors/
portalPopularReports=/var/local/monitoring/output/portalPopularReports/
portalUserReports=/var/local/monitoring/output/portalUserReports/
portalUserDuration=/var/local/monitoring/output/portalUserLogs/
portalUserLogs=/var/local/monitoring/output/portalUserLogs/
sdkExportJobStats=/var/local/monitoring/output/exportJobStats/
cctExportJobStats=/var/local/monitoring/output/exportJobStats/
ceiIndex=/var/local/monitoring/output/ceiIndex/
yarnQueue=/var/local/monitoring/output/capacityScheduler/
confPath=/opt/nsn/ngdb/monitoring/conf/
schedulerServicePath=/opt/nsn/ngdb/scheduler/app/bin/
sparkMonitoring=/var/local/monitoring/output/sparkMonitoring/
pySparkMonitoring=/var/local/monitoring/output/sparkMonitoring/
[PostgresSqlSection]
actualQsDimensionquery = select lower(SUBSTRING(table_name, 4,100)) as TableName,count(*) as ActualCount from cache_tab where source='D' and last_fetched_time >= 'startTime' and table_name like '%es_%' group by (table_name) order by lower(table_name) desc
expectedQsDimensionquery = select lower(SUBSTRING(SUBSTRING(job_name, 8, 45),1,length(SUBSTRING(job_name, 8, 45))-15)) as TableName,count(*) as ExpectedCount from ws_dimension_query_tab where job_name like '%Entity%' group by (job_name) order by lower(job_name) desc;
lastUptimeQuery=select json_extract_path_text(hm_json,'Date') from sairepo.hm_stats where hm_type='ServiceStability' and json_extract_path_text(hm_json,'Service') like '%service_name%' and json_extract_path_text(hm_json,'Last_Restart_Time') not like '%Service is down%' order by json_extract_path_text(hm_json,'Date') desc limit 1
exportJobLocationQuery=select a.jobid,b.paramvalue from sairepo.job_dictionary a inner join (select jobid,paramvalue from sairepo.job_prop where jobid in (select jobid from sairepo.job_prop  where jobid in (select id from sairepo.job_dictionary where jobid like '%Export%') and paramname='HIVE_TO_HBASE_LOADER' and paramvalue='false') and paramname='EXPORT_LOCATIONS') b on a.id=b.jobid;
cei_index_query=select hm_json from sairepo.hm_stats where hm_type='ceiIndex' and json_extract_path_text(hm_json,'Date')='DATE' order by json_extract_path_text(hm_json,'Presentation Name') asc
ceiTrendsResponse=select response from saiws.cache_tab where request like '%CEI Trends%' and query not like '%technology%' and table_name like '%cei2_o_index_city%day%' and source='S' and dt='Date'
getHungJobs=select proc_id, job_name from sairepo.etl_status where end_time is NULL and status='R' and job_name like '%job_type%Agg%' and job_name in (select job_id from sairepo.to_execute where to_execute like '%SubPartitionValues%') and DATE_PART('HOUR', (NOW() AT TIME ZONE 'time_zone' - start_time)) >= 'threshold' order by proc_id, start_time
updateStatusToE=update sairepo.etl_status set status='E' where proc_id='procId' and job_name='jobName' and status='R'
getFailJobGroup=select job_group from saischedule.qrtz_job_details where job_name='jobName'
[hiveSqlSection]
zeroSizePartitionsquery=select * from "PARTITIONS" p join "PARTITION_PARAMS" pp on p."PART_ID"=pp."PART_ID" and pp."PARAM_KEY"='numRows' and pp."PARAM_VALUE"='0' and p."TBL_ID"=(select "TBL_ID" from "TBLS" where "TBL_NAME"='table_name');
deletePartition_Params=delete from "PARTITION_PARAMS" where "PART_ID" in (select "PART_ID" from "PARTITIONS" where "TBL_ID" in (select "TBL_ID" from "TBLS" where "TBL_NAME"='table_name') and "PART_NAME" = 'partition');
deletePartition_Key_Vals=delete from "PARTITION_KEY_VALS" where "PART_ID" in (select "PART_ID" from "PARTITIONS" where "TBL_ID" in (select "TBL_ID" from "TBLS" where "TBL_NAME"='table_name') and "PART_NAME" = 'partition');
deletePartitions=delete from "PARTITIONS" where "TBL_ID" in (select "TBL_ID" from "TBLS" where "TBL_NAME"='table_name') and "PART_NAME" = 'partition';
getHiveTableDir=select "LOCATION" from "SDS" where "SD_ID"=(select "SD_ID" from "TBLS" where "TBL_NAME"='table_name');
getTables=select "TBL_NAME" from "TBLS" where "TBL_ID" in (select "TBL_ID" from "PARTITION_KEYS" group by "TBL_ID" having count(*)>2) and "TBL_NAME" not like '%tnp%';
showtablesquery=show tables
columnquery=show columns from tablename
totalCountQuery=select count(*) from tablename where dt='startpartition'
columnCountQuery=select count(*), count(distinct column) from tablename where dt='startpartition'
[HeaderSection]
HdfsUsage=Time,FileSystem,Size,Used,Available,Use(%)
NgdbFileSystem=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
TotalFileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
ServiceStability=Date,Host,Service,Last_Restart_Time,Uptime,Downtime
PortalStability=Date,Host,Service,Last_Restart_Time,Uptime,Downtime
NameNodeStats=Time,Node,Reachability_Status,CPU_USE%,MEM_USE%,DISK_USE%,DFSZKFailover Node,HA proxy,Hadoop Name Node,Hbase Master,Hive MetaStore,Hive Server,Postgres HIVE,Journal Node,NameNode,Resource Manger
AppNodesStats=Time,Node,Reachability_Status,CPU_USE%,MEM_USE%,DISK_USE%,Analytics WebService,BoxiWebiServer,Boxi-Adaptive,Boxi-CMC Tomcat Service,Boxi-Web Application service,Couchbase,Kafka Broker,Kafka UI,Kafka Zookeeper,Kibana,Postgres-Analytics,Postgres SDK,Spark Thrift App,Spark Thrift Flexi,Spark Thrift Portal
DataNodeStats=Time,Node,Reachability_Status,CPU_USE%,MEM_USE%,DISK_USE%,Hadoop Data Node,Hbase Region server,Node Manager
ServiceStats=Time,Service,Host,Status,CPU_USE,TotalMEM,OpenFiles
Nodes_Status=Time,Nodes_Type,Total_Nodes,Nodes_Up,Nodes_down,Remarks
etlAdaptationProp=Time,adaptation_name,adaptation_dir,total_files,small_files_count,large_files_count
PortalStats=Time,Service,Host,Status,CPU_USE,TotalMEM,OpenFiles
statsForServices=Scheduler,Webservice,Dal,Analytics WebService,Portal Tomcat,Portal HTTPD,Portal Postgres
TopologyStats=Time,Node,Reachability_Status,Topology,Topology_Status,Connector_type,CPU_USE,TotalMEM,OpenFiles
SourceSinkTasksStatus=Time,Connector_Type,Connector_Status,Tasks_Status,Active_Tasks,Total_Tasks,Connector_Running_On_Node,Tasks_Running_on_Nodes
DataTraffic=Date,total_data_volume,number_of_calls,volte_call_attempts,vowifi_call_attempts,SMS_Active_Subscribers,VOICE_Active_Subscribers,VOLTE_Active_Subscribers,VoWIFI_Active_Subscribers,Data_CP_Active_Subscribers,Data_UP_Active_Subscribers
DiskUsage=Time, Node, DiskName, UtilizedCapacity
AllNodesFileSystemUsage=Time,Node,Disk_Size(GB),Disk_Used(GB),Disk_Available(GB),Percentage_Disk_Used
MemoryUsage=Time, Node, Free Mem(Gb), BufferCached(Gb)
CpuUsage=Time, Node, User Cpu, System Cpu, Idle Cpu, Io wait
CheckTimeSpansInFile=Time,Topology,Topology_dir,Total_files_fetched,Time_spans_in_file,min_time,max_time
NgdbFileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
NgdbEsFileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
NgdbPsFileSystemUsage_15MIN=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
NgdbPsFileSystemUsage_HOUR=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
NgdbPsFileSystemUsage_DAY=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
NgdbPsFileSystemUsage_WEEK=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
NgdbPsFileSystemUsage_MONTH=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
HbaseFileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
HbaseWALsFileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
Hbasehbase-stagingFileSystemUsage=Time,Directory,Usage_with_replication(GB),Usage_without_replication(GB)
JobsFailure=Time, Job Name,Start Time, End Time, Status, Error Description
DataSourceReachability=Time,Host,Status
TnpLatency=JobName,StartTime,EndTime,TnpLatency,OverallLatency(in minutes)
DimensionCount=Date,Dimension Table,Count
boundaryStatus=time_frame,jobid,maxvalue
boundaryStatusUsage=time_frame,jobid,maxvalue,region_id
postgresUsersCount=time,username,count
failedQsTable=jobname,count
expectedActualQsTable=TableName,ExpectedCount,ActualCount
boundaryStatusReagg=time_frame,jobname,status,maxvalue
openFiles=Date,ProcessId,#ofOpenFiles
openFilesDebug=Date,ProcessId,ProcessName
openFilesDetails=Date,ProcessId,ProcessName,#ofOpenFiles
etlTopologiesLag=Time,Connector,LagCount(Messages),LagCount(Records)
hdfsPerformance=Date,hdfsreadtime,hdfswritetime
webserviceMemory=Date,Host,TotalMemory(Mb)
couchBaseStats=Date,BucketName,ItemCount,Ram Usage(MB),Ram Quota(MB),Data Usage(MB),Disk Usage(MB),Ram Usage(%)
couchBaseRamSummary=Date,Total Ram Used(MB),Total Ram Quota(MB), Ram Perc(%)
couchBaseCpuSummary=Date,Node Name,Cpu Utilization,Swap Total,Swap Used,Total Memory,Free Memory
queriesPerClient=Date,Node,ClientIP,TotalQueries,QueriesWith401,QueriesWith403
portalConnections=Date,Host,LoadBalancer,PortalService,PortalDb,
portalTomcat=Date,Host,Memory
webserviceTomcat=Date,Host,Memory
portalMemory=Date,Host,TotalMemory,FreeMemory,FreeBufferCache
portalQueries=Date,Host,Organisation,TotalQueries,CacheQueries,DbQueries,NoDataQueries,WSException
portalNoDataErrors=Date,Host,Organisation,ErrorDescription
portalAuditLogErrors=Date,Host,ErrorCode,ErrorDescription
portalPopularReports=Date,UseCase,Count
portalUserReports=Date,User,UseCase,Count
portalUserDuration=Date,Host,Organisation,User,IP,Duration
portalUserLogs=Date,Host,Organisation,User,IP,Login,Logout
sdkExportJobStats=Date,Export Job,No. of Files,Size
cctExportJobStats=Date,Export Job,No. of Files,Size
ceiIndex=Date,Presentation Name,Index Value
dayTableCount=Date,Table,TotalCount,Column,DistinctCountOfSelectedColumn,VarFactor
monthTableCount=Date,Table,TotalCount,Column,DistinctCountOfSelectedColumn,VarFactor
weekTableCount=Date,Table,TotalCount,Column,DistinctCountOfSelectedColumn,VarFactor
sparkMonitoring=Date,Application Id,Query,Start Time,End Time,Status
pySparkMonitoring=Date,Application Id,JobName,Start Time,End Time,Duration
[ColumnSection]
DataTraffic=SUM(total_data_volume);SUM(number_of_calls);SUM(volte_call_attempts);SUM(vowifi_call_attempts);count(distinct case when NUMBER_OF_SMS > 0 then imsi_id else NULL end) as SMS_Active_Subscribers;count(distinct case when NUMBER_OF_CALLS > 0 then imsi_id else NULL end) as VOICE_Active_Subscribers;count(distinct case when nvl(VOLTE_CALL_ATTEMPTS,0) +  nvl(VOLTE_REG_ATTEMPTS,0) +  nvl(VOLTE_BEARER_ACT_EVENTS,0) + nvl(VOLTE_DEDICATED_ACT_EVENTS,0) + nvl(SRVCC_ATTEMPTS,0)+nvl(volte_re_registration_attempts,0) > 0 then imsi_id else NULL end) as VOLTE_Active_Subscribers;count(distinct case when technology = 'WiFi' then imsi_id else NULL end) as VoWIFI_Active_Subscribers;count(distinct case when DATA_CP_RECORD_COUNT > 0 then imsi_id else NULL end) as Data_CP_Active_Subscribers;count(distinct case when EVENT_COUNT > 0 then imsi_id else NULL end) as Data_UP_Active_Subscribers
etl_topologies=select specid, mnt_column, ngdb_column, usage_job, plevel, right2.maxvalue from (select us.specid,ac.adaptationid||'_'||ac.version||'/'||us.specid||'_'||us.version as mnt_column,ac.adaptationid||'_'||ac.version||'/'||case when aus.specid is null then us.specid else aus.specid end||'_'||case when aus.specid is null then us.version else aus.version end as ngdb_column, 'Usage_'||case when aus.specid is null then us.specid else aus.specid end||'_1_LoadJob' usage_job  from (select adaptationid as aer_adapid,usagespecid as aer_usid from adap_entity_rel where adaptationid in (select id from adapt_cata) and usagespecid is not null) join_table  join adapt_cata ac on join_table.aer_adapid=ac.id join (select * from usage_spec where (abstract is null or abstract ='TRANSIENT') and version = '1') us on join_table.aer_usid=us.id left outer join usage_spec_rel usr on usr.transientspec=us.id left outer join usage_spec aus on aus.id=usr.virtualspec and aus.version = '1' order by us.specid)left1 join (select jd.jobid job, jp.paramvalue plevel from job_dictionary jd join job_prop jp on jd.id=jp.jobid and jp.paramname='PLEVEL' and jd.jobid like 'Usage%LoadJob')right1 on left1.usage_job=right1.job left outer join (select boundary.jobid as jobid,maxvalue from boundary where jobid like '%Usage%') right2 on left1.usage_job=right2.jobid;